{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fcf8b",
   "metadata": {},
   "source": [
    "# Weighted Ensemble Method\n",
    "\n",
    "Used models (for the specific setup see file \"models.py\"):\n",
    "    - Pretrained ResNet18, trained entire model\n",
    "    - Non-pretrained ResNet18, trained entire model\n",
    "    - Pretrained AlexNet, trained entire model\n",
    "    - Pretrained SqueezeNet, trained entire model\n",
    "    \n",
    "Each model is ran in a separate cell. Two cells below this one we set up all the hyperparameters that seemed optimal from the weights and biases platform hyperparameter optimization. Due to the relative differences in accuracies/ROC-AUC between the models we decided to assign different weights to different models dependent on their relative accuracies, more on this in the last cell. The main idea behind using such a weighted ensemble method is generalization. This way any biases that might be present in a single model but not in the others get eliminated due to each model having only a relatively small influence on the final predictions.\n",
    "\n",
    "Three of the four models were pretrained on ImageNet, which might introduce biases present in that dataset. To reduce these biases, we used a non-pretrained ResNet18 as fourth model.\n",
    "\n",
    "At the end of the notebook we load all of the softmax predictions from csv files that led to the final submission. These softmax predictions are the outputs from the models described above and .\n",
    "\n",
    "Note that we initially tried to include a GoogleNet as well but due to the fact that it prerformed badly we removed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pycm import *\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from models import *\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c031249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Optimal parameters as found by the hyperparameter optimization in the weights and biases platform.\n",
    "\n",
    "\"\"\"\n",
    "alex_batch_size = 64\n",
    "res_batch_size = 64\n",
    "squeeze_batch_size = 128\n",
    "\n",
    "res_momentum = 0.8\n",
    "squeeze_momentum = 0.9\n",
    "\n",
    "alex_epochs = 40\n",
    "res_epochs = 30\n",
    "squeeze_epochs = 30\n",
    "\n",
    "res_weight_decay = 1e-2\n",
    "squeeze_weight_decay = 0.001\n",
    "\n",
    "alex_lr = 0.0005\n",
    "res_lr = 1e-2\n",
    "squeeze_lr = 1e-2\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba48214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Setting up the datasets and transforms. \n",
    "\n",
    "These are just the basic transforms, different models might use different transforms.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Optimal transforms as found by data augmentation tests - see presentation and other notebook for more details\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5094, 0.5094, 0.5094], [0.2314, 0.2314, 0.2314]),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.1),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.Resize(224)\n",
    "                                     ])\n",
    "\n",
    "# Test transforms should only include 'preprocessing', no data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5094, 0.5094, 0.5094], [0.2314, 0.2314, 0.2314]),\n",
    "                                     transforms.Resize(224)\n",
    "                                    ])\n",
    "\n",
    "covid_train_full = datasets.ImageFolder('xray-data/train', transform=train_transform)\n",
    "covid_test = covid_test = TestDataSet('xray-data/test', transform=test_transform)\n",
    "test_loader = DataLoader(covid_test , batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ac343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "AlexNet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate the dataloader\n",
    "train_loader = DataLoader(covid_train_full, batch_size=alex_batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "# Loading in the model\n",
    "model = CustomAlexnet().to(device)\n",
    "\n",
    "# Setting up optimizer and loss function, Adam turned out to be best for AlexNet\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alex_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare model for training\n",
    "model.train()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "# Training subroutine, training on full dataset\n",
    "for epoch in range(alex_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader, device=device, imshape=(-1, 3, 224, 224))\n",
    "\n",
    "    # Update the logs for the training data\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "    liveloss.update(logs)\n",
    "\n",
    "    liveloss.draw()\n",
    "\n",
    "# Get predictions (softmax)\n",
    "preds_alex = predict(model, test_loader, max=False, imshape=(-1, 3, 224, 224), device=device)\n",
    "\n",
    "# Save predictions to a file\n",
    "preds_to_file(preds_alex, \"preds_alex\")\n",
    "\n",
    "# Deleting the model to free up GPU memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a485afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "SqueezeNet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate the dataloader\n",
    "train_loader = DataLoader(covid_train_full, batch_size=squeeze_batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "# Loading in the model\n",
    "model = CustomSqueezenet().to(device)\n",
    "\n",
    "# Setting up optimizer and loss function, we included adaptive learning rate. We investigated the effects of this\n",
    "# adaptive learning rate, and in the end decided to just use a very small learning rate for the final submission\n",
    "# which achieved the same thing. We left the adaptive learning rate in here for presentation purposes.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=squeeze_lr, weight_decay=squeeze_weight_decay, momentum=squeeze_momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Prepare model for training\n",
    "model.train()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "# Training subroutine, training on full dataset\n",
    "for epoch in range(squeeze_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader, device=device, imshape=(-1, 3, 224, 224))\n",
    "\n",
    "    # Update the logs for the training data\n",
    "    scheduler.step()\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "    liveloss.update(logs)\n",
    "\n",
    "    liveloss.draw()\n",
    "    \n",
    "# Get predictions (softmax)\n",
    "preds_squeeze = predict(model, test_loader, max=False, imshape=(-1, 3, 224, 224), device=device)\n",
    "\n",
    "# Save predictions to a file\n",
    "preds_to_file(preds_squeeze, \"preds_squeeze\")\n",
    "\n",
    "# Deleting the model to free up GPU memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ResNet\n",
    "\n",
    "Note that we only included the pretrained version of the resnet here that we used for the final results for \n",
    "presentation purposes. It is probably not interesting to have two of the exact same models in here.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# The resnet uses a single input channel, which is why the transforms are a little different\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.Normalize([0.5094], [0.2314]),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.1),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.Resize(224)\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.Normalize([0.5094], [0.2314]),\n",
    "                                     transforms.Resize(224)\n",
    "                                    ])\n",
    "\n",
    "# Setting the transforms on the data\n",
    "covid_train_full.transform = train_transform\n",
    "covid_test.transform = test_transform\n",
    "\n",
    "# Making the dataloaders again\n",
    "train_loader = DataLoader(covid_train_full, batch_size=res_batch_size, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(covid_test, batch_size=1, shuffle=False, num_workers=1, drop_last=False)\n",
    "\n",
    "\n",
    "model = CustomResnet().to(device)\n",
    "\n",
    "# Setting up optimizer and loss function, Adam turned out to be best for AlexNet\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=res_lr, momentum=res_momentum)  # define an optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare model for training\n",
    "model.train()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "# Training subroutine, training on full dataset\n",
    "for epoch in range(res_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader, device=device, imshape=(-1, 1, 224, 224))\n",
    "\n",
    "    # Update the logs for the training data\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "    liveloss.update(logs)\n",
    "\n",
    "    liveloss.draw()\n",
    "\n",
    "# Get predictions (softmax)\n",
    "preds_res = predict(model, test_loader, max=False, imshape=(-1, 1, 224, 224), device=device)\n",
    "\n",
    "# Save predictions to a file\n",
    "preds_to_file(preds_res, \"preds_res\")\n",
    "\n",
    "# Deleting the model to free up GPU memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5159bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Execute this cell if predictions are not anymore in kernel's variables.\n",
    "\n",
    "THIS CELL WILL NOT RUN IF ABOVE CELLS ARE NOT RAN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "preds_alex = preds_from_file(\"preds_alex\")\n",
    "preds_squeeze = preds_from_file(\"preds_squeeze\")\n",
    "preds_res = preds_from_file(\"preds_res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Get a weighted average of predictions by different models\n",
    "\n",
    "THIS CELL WILL NOT RUN IF ABOVE CELLS ARE NOT RAN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "preds_average([preds_alex, preds_res, preds_squeeze], [1.2, 1, 2]) # The predictions and the respective weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07325365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "For reproducibility of the final results, here are the softmax outputs of the models described above. Run this cell\n",
    "to get our predictions for our best submission. No need to run above cells before this, except for the imports.\n",
    "\n",
    "- \"softmax_alexnet.csv\": Pretrained AlexNet\n",
    "- \"softmax_resnet18_1.csv\": Non-pretrained ResNet18\n",
    "- \"softmax_resnet18_2.csv\": Pretrained ResNet18\n",
    "- \"softmax_squeezenet.csv\": Pretrained SqueezeNet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "preds_alex = preds_from_file(\"predictions/softmax_alexnet.csv\")\n",
    "preds_squeeze = preds_from_file(\"predictions/softmax_squeezenet.csv\")\n",
    "preds_res_1 = preds_from_file(\"predictions/softmax_resnet18_1.csv\")\n",
    "preds_res_2 = preds_from_file(\"predictions/softmax_resnet18_2.csv\")\n",
    "\n",
    "# Why did we choose these weights? \n",
    "# 1. Trial and error based on accuracies in validation sets\n",
    "# 2. Intuition based on what we wanted to achieve. SqueezeNet and AlexNet had low generalization error. Resnets\n",
    "#    had more generalization error but achieved higher accuries. The ResNet18_1 was mostly there to relieve biases\n",
    "#    from the pretrained models.\n",
    "preds_new = preds_average([preds_res_1, preds_alex, preds_squeeze, preds_res_2], [0.5, 5, 5, 0.1])\n",
    "\n",
    "# Our team dropped one place in the private leaderboard with respect to the public leaderboard. This might be an\n",
    "# indication for these weights being overfit to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49e15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
